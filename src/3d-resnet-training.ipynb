{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mediapipe opencv-python-headless tqdm torch cupy-cuda11x","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:31:58.809310Z","iopub.execute_input":"2024-12-03T11:31:58.809652Z","iopub.status.idle":"2024-12-03T11:32:14.501738Z","shell.execute_reply.started":"2024-12-03T11:31:58.809622Z","shell.execute_reply":"2024-12-03T11:32:14.500528Z"}},"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting cupy-cuda11x\n  Downloading cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (23.2.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (24.3.25)\nRequirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.4.26.dev20240620)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mediapipe) (3.7.5)\nRequirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from mediapipe) (1.26.4)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.10/site-packages (from mediapipe) (4.10.0.84)\nCollecting protobuf<5,>=4.25.3 (from mediapipe)\n  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\nCollecting sounddevice>=0.4.4 (from mediapipe)\n  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from mediapipe) (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.10/site-packages (from cupy-cuda11x) (0.8.2)\nRequirement already satisfied: CFFI>=1.0 in /opt/conda/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (0.3.2)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax->mediapipe) (1.14.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nDownloading mediapipe-0.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl (96.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\nInstalling collected packages: protobuf, cupy-cuda11x, sounddevice, mediapipe\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.5 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.5 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.5 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cupy-cuda11x-13.3.0 mediapipe-0.10.18 protobuf-4.25.3 sounddevice-0.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall cupy cupy-cuda11x -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:57:53.988539Z","iopub.execute_input":"2024-12-03T10:57:53.988867Z","iopub.status.idle":"2024-12-03T10:57:56.591656Z","shell.execute_reply.started":"2024-12-03T10:57:53.988841Z","shell.execute_reply":"2024-12-03T10:57:56.590793Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: cupy 13.3.0\nUninstalling cupy-13.3.0:\n  Successfully uninstalled cupy-13.3.0\nFound existing installation: cupy-cuda11x 13.3.0\nUninstalling cupy-cuda11x-13.3.0:\n  Successfully uninstalled cupy-cuda11x-13.3.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install cupy-cuda11x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:58:10.198946Z","iopub.execute_input":"2024-12-03T10:58:10.199424Z","iopub.status.idle":"2024-12-03T10:58:21.832317Z","shell.execute_reply.started":"2024-12-03T10:58:10.199382Z","shell.execute_reply":"2024-12-03T10:58:21.831461Z"}},"outputs":[{"name":"stdout","text":"Collecting cupy-cuda11x\n  Using cached cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: numpy<2.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from cupy-cuda11x) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /opt/conda/lib/python3.10/site-packages (from cupy-cuda11x) (0.8.2)\nUsing cached cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl (96.6 MB)\nInstalling collected packages: cupy-cuda11x\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cupy-cuda11x-13.3.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\n# Initialize MediaPipe Pose\nmp_pose = mp.solutions.pose\npose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\ndef preprocess_video(video_path, output_folder):\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    preprocessed_data = []\n    \n    for frame_idx in tqdm(range(frame_count)):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Convert the image to RGB and process it with MediaPipe\n        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = pose.process(image_rgb)\n        \n        if results.pose_landmarks:\n            landmarks = results.pose_landmarks.landmark\n            \n            # Extract relevant landmarks (e.g., shoulders, elbows, wrists)\n            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n                          landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n            right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n                           landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n                          landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n            right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n                           landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n            \n            # Calculate angles\n            left_arm_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n            right_arm_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n            \n            preprocessed_data.append({\n                'frame': frame_idx,\n                'left_arm_angle': left_arm_angle,\n                'right_arm_angle': right_arm_angle,\n                'landmarks': [(landmark.x, landmark.y, landmark.z) for landmark in landmarks]\n            })\n    \n    cap.release()\n    \n    # Save preprocessed data\n    np.save(os.path.join(output_folder, f\"{os.path.basename(video_path)}_preprocessed.npy\"), preprocessed_data)\n\ndef calculate_angle(a, b, c):\n    a = np.array(a)\n    b = np.array(b)\n    c = np.array(c)\n    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n    angle = np.abs(radians * 180.0 / np.pi)\n    if angle > 180.0:\n        angle = 360 - angle\n    return float(angle)\n\n# Main preprocessing loop\ninput_folder = '/kaggle/input/pushups-all2/pushups_all_2'\noutput_folder = '/kaggle/working/preprocessed_data'\nos.makedirs(output_folder, exist_ok=True)\n\nfor video_file in os.listdir(input_folder):\n    if video_file.endswith('.mp4'):\n        video_path = os.path.join(input_folder, video_file)\n        preprocess_video(video_path, output_folder)\n\nprint(\"Preprocessing complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:33:05.479746Z","iopub.execute_input":"2024-12-03T11:33:05.480488Z","iopub.status.idle":"2024-12-03T11:37:25.065024Z","shell.execute_reply.started":"2024-12-03T11:33:05.480453Z","shell.execute_reply":"2024-12-03T11:37:25.064085Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/212 [00:00<?, ?it/s]W0000 00:00:1733225585.603161     115 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1733225585.660109     115 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1733225585.683148     114 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n100%|██████████| 212/212 [00:05<00:00, 40.26it/s]\n100%|██████████| 58/58 [00:01<00:00, 43.28it/s]\n100%|██████████| 172/172 [00:03<00:00, 43.22it/s]\n100%|██████████| 113/113 [00:02<00:00, 38.49it/s]\n100%|██████████| 83/83 [00:02<00:00, 40.69it/s]\n100%|██████████| 148/148 [00:03<00:00, 42.05it/s]\n100%|██████████| 156/156 [00:03<00:00, 41.37it/s]\n100%|██████████| 75/75 [00:01<00:00, 41.19it/s]\n100%|██████████| 93/93 [00:02<00:00, 44.95it/s]\n100%|██████████| 79/79 [00:01<00:00, 41.53it/s]\n100%|██████████| 60/60 [00:01<00:00, 42.51it/s]\n100%|██████████| 75/75 [00:01<00:00, 41.63it/s]\n100%|██████████| 70/70 [00:01<00:00, 42.11it/s]\n100%|██████████| 290/290 [00:06<00:00, 43.46it/s]\n100%|██████████| 63/63 [00:01<00:00, 43.08it/s]\n100%|██████████| 54/54 [00:01<00:00, 40.28it/s]\n100%|██████████| 95/95 [00:02<00:00, 42.00it/s]\n100%|██████████| 66/66 [00:01<00:00, 43.57it/s]\n100%|██████████| 119/119 [00:02<00:00, 42.97it/s]\n100%|██████████| 83/83 [00:01<00:00, 43.40it/s]\n100%|██████████| 107/107 [00:02<00:00, 41.82it/s]\n100%|██████████| 71/71 [00:01<00:00, 42.00it/s]\n100%|██████████| 90/90 [00:02<00:00, 42.30it/s]\n100%|██████████| 76/76 [00:01<00:00, 42.46it/s]\n100%|██████████| 72/72 [00:01<00:00, 44.08it/s]\n100%|██████████| 110/110 [00:02<00:00, 44.43it/s]\n100%|██████████| 74/74 [00:01<00:00, 40.81it/s]\n100%|██████████| 93/93 [00:02<00:00, 43.05it/s]\n100%|██████████| 137/137 [00:03<00:00, 41.19it/s]\n100%|██████████| 110/110 [00:02<00:00, 39.12it/s]\n100%|██████████| 115/115 [00:02<00:00, 41.57it/s]\n100%|██████████| 52/52 [00:01<00:00, 44.87it/s]\n100%|██████████| 87/87 [00:02<00:00, 41.93it/s]\n100%|██████████| 124/124 [00:02<00:00, 41.56it/s]\n100%|██████████| 58/58 [00:01<00:00, 43.77it/s]\n100%|██████████| 45/45 [00:01<00:00, 43.23it/s]\n100%|██████████| 232/232 [00:05<00:00, 42.63it/s]\n100%|██████████| 61/61 [00:01<00:00, 44.44it/s]\n100%|██████████| 128/128 [00:02<00:00, 44.52it/s]\n100%|██████████| 131/131 [00:03<00:00, 42.44it/s]\n100%|██████████| 324/324 [00:07<00:00, 40.58it/s]\n100%|██████████| 100/100 [00:02<00:00, 43.24it/s]\n100%|██████████| 107/107 [00:02<00:00, 42.41it/s]\n100%|██████████| 397/397 [00:10<00:00, 39.65it/s]\n100%|██████████| 84/84 [00:02<00:00, 40.03it/s]\n100%|██████████| 100/100 [00:02<00:00, 42.79it/s]\n100%|██████████| 91/91 [00:02<00:00, 41.01it/s]\n100%|██████████| 47/47 [00:01<00:00, 44.31it/s]\n100%|██████████| 75/75 [00:01<00:00, 40.30it/s]\n100%|██████████| 72/72 [00:01<00:00, 39.54it/s]\n100%|██████████| 84/84 [00:02<00:00, 41.00it/s]\n100%|██████████| 153/153 [00:03<00:00, 42.39it/s]\n100%|██████████| 63/63 [00:01<00:00, 45.09it/s]\n100%|██████████| 90/90 [00:02<00:00, 43.04it/s]\n100%|██████████| 75/75 [00:01<00:00, 42.26it/s]\n100%|██████████| 92/92 [00:02<00:00, 41.16it/s]\n100%|██████████| 66/66 [00:01<00:00, 43.96it/s]\n100%|██████████| 53/53 [00:01<00:00, 44.19it/s]\n100%|██████████| 83/83 [00:01<00:00, 44.47it/s]\n100%|██████████| 64/64 [00:01<00:00, 44.43it/s]\n100%|██████████| 230/230 [00:05<00:00, 41.92it/s]\n100%|██████████| 107/107 [00:02<00:00, 43.46it/s]\n100%|██████████| 100/100 [00:02<00:00, 43.77it/s]\n100%|██████████| 77/77 [00:01<00:00, 41.33it/s]\n100%|██████████| 133/133 [00:03<00:00, 40.69it/s]\n100%|██████████| 87/87 [00:02<00:00, 41.33it/s]\n100%|██████████| 61/61 [00:01<00:00, 44.37it/s]\n100%|██████████| 123/123 [00:02<00:00, 41.47it/s]\n100%|██████████| 119/119 [00:02<00:00, 44.32it/s]\n100%|██████████| 125/125 [00:02<00:00, 43.91it/s]\n100%|██████████| 250/250 [00:05<00:00, 44.11it/s]\n100%|██████████| 157/157 [00:03<00:00, 43.73it/s]\n100%|██████████| 68/68 [00:01<00:00, 40.63it/s]\n100%|██████████| 77/77 [00:01<00:00, 42.05it/s]\n100%|██████████| 105/105 [00:02<00:00, 43.93it/s]\n100%|██████████| 230/230 [00:05<00:00, 41.57it/s]\n100%|██████████| 91/91 [00:02<00:00, 41.16it/s]\n100%|██████████| 100/100 [00:02<00:00, 44.69it/s]\n100%|██████████| 211/211 [00:05<00:00, 41.85it/s]\n100%|██████████| 116/116 [00:02<00:00, 42.40it/s]\n100%|██████████| 81/81 [00:01<00:00, 41.62it/s]\n100%|██████████| 85/85 [00:01<00:00, 44.64it/s]\n100%|██████████| 57/57 [00:01<00:00, 44.26it/s]\n100%|██████████| 77/77 [00:01<00:00, 42.11it/s]\n100%|██████████| 60/60 [00:01<00:00, 45.19it/s]\n100%|██████████| 121/121 [00:02<00:00, 44.51it/s]\n100%|██████████| 126/126 [00:02<00:00, 44.37it/s]\n100%|██████████| 158/158 [00:03<00:00, 42.60it/s]\n100%|██████████| 121/121 [00:02<00:00, 42.04it/s]\n100%|██████████| 107/107 [00:02<00:00, 41.47it/s]\n100%|██████████| 126/126 [00:02<00:00, 45.54it/s]\n100%|██████████| 98/98 [00:02<00:00, 44.42it/s]\n100%|██████████| 153/153 [00:03<00:00, 42.16it/s]\n100%|██████████| 88/88 [00:02<00:00, 41.98it/s]\n100%|██████████| 104/104 [00:02<00:00, 45.12it/s]\n100%|██████████| 62/62 [00:01<00:00, 41.89it/s]\n100%|██████████| 99/99 [00:02<00:00, 44.11it/s]\n100%|██████████| 146/146 [00:03<00:00, 41.57it/s]\n100%|██████████| 69/69 [00:01<00:00, 42.97it/s]\n100%|██████████| 66/66 [00:01<00:00, 44.73it/s]","output_type":"stream"},{"name":"stdout","text":"Preprocessing complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pwd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:52:01.340477Z","iopub.execute_input":"2024-12-03T11:52:01.341230Z","iopub.status.idle":"2024-12-03T11:52:02.351812Z","shell.execute_reply.started":"2024-12-03T11:52:01.341195Z","shell.execute_reply":"2024-12-03T11:52:02.350862Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!ls -ltra ./preprocessed_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:52:05.720549Z","iopub.execute_input":"2024-12-03T11:52:05.721457Z","iopub.status.idle":"2024-12-03T11:52:06.749197Z","shell.execute_reply.started":"2024-12-03T11:52:05.721405Z","shell.execute_reply":"2024-12-03T11:52:06.748056Z"}},"outputs":[{"name":"stdout","text":"total 12248\ndrwxr-xr-x 4 root root   4096 Dec  3 11:32 ..\n-rw-r--r-- 1 root root 239830 Dec  3 11:33 wrong28.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  65348 Dec  3 11:33 correct40.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 194510 Dec  3 11:33 wrong36.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 127663 Dec  3 11:33 wrong46.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  93673 Dec  3 11:33 correct34.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 167318 Dec  3 11:33 correct28.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 176382 Dec  3 11:33 correct32.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  84609 Dec  3 11:33 correct6.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 105003 Dec  3 11:33 wrong13.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  89141 Dec  3 11:33 correct4.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  67614 Dec  3 11:33 correct39.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  84609 Dec  3 11:33 correct49.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  77811 Dec  3 11:33 correct30.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 328239 Dec  3 11:33 wrong1.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  71013 Dec  3 11:33 correct44.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  60816 Dec  3 11:33 correct33.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 107269 Dec  3 11:33 correct37.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  74412 Dec  3 11:33 wrong23.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 134461 Dec  3 11:33 wrong20.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  93673 Dec  3 11:33 wrong41.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 120865 Dec  3 11:33 correct16.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  80077 Dec  3 11:34 correct9.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 101604 Dec  3 11:34 correct15.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  85742 Dec  3 11:34 correct5.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  81210 Dec  3 11:34 wrong18.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 124264 Dec  3 11:34 wrong21.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  83476 Dec  3 11:34 correct10.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 105003 Dec  3 11:34 wrong50.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 154855 Dec  3 11:34 correct48.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 124264 Dec  3 11:34 correct26.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 129929 Dec  3 11:34 correct21.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  58550 Dec  3 11:34 correct35.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  98205 Dec  3 11:34 correct11.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 140126 Dec  3 11:34 correct12.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  65348 Dec  3 11:34 correct41.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  50619 Dec  3 11:34 correct42.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 262490 Dec  3 11:34 wrong24.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  68747 Dec  3 11:34 correct46.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 144658 Dec  3 11:34 wrong16.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 148057 Dec  3 11:34 correct25.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 366795 Dec  3 11:34 wrong29.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 112934 Dec  3 11:34 correct22.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 120865 Dec  3 11:34 correct19.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 449577 Dec  3 11:35 wrong39.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  94806 Dec  3 11:35 correct2.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 112934 Dec  3 11:35 wrong7.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 102737 Dec  3 11:35 correct13.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  52885 Dec  3 11:35 wrong35.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  84609 Dec  3 11:35 wrong48.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  81210 Dec  3 11:35 correct7.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  94806 Dec  3 11:35 wrong49.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 172983 Dec  3 11:35 wrong10.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  71013 Dec  3 11:35 wrong30.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 101604 Dec  3 11:35 wrong38.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  84609 Dec  3 11:35 correct8.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 103870 Dec  3 11:35 correct14.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  74412 Dec  3 11:35 wrong42.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  59683 Dec  3 11:35 correct38.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  93673 Dec  3 11:35 wrong3.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  72146 Dec  3 11:35 wrong33.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 260224 Dec  3 11:35 wrong27.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 120865 Dec  3 11:35 wrong9.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 112934 Dec  3 11:35 wrong45.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  86875 Dec  3 11:35 correct45.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 150323 Dec  3 11:35 correct18.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  98205 Dec  3 11:35 wrong22.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  68747 Dec  3 11:35 correct1.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 138993 Dec  3 11:35 correct17.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 134461 Dec  3 11:36 wrong19.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 141259 Dec  3 11:36 wrong6.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 282884 Dec  3 11:36 wrong15.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 177515 Dec  3 11:36 correct43.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  76678 Dec  3 11:36 correct29.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  86875 Dec  3 11:36 correct23.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 118599 Dec  3 11:36 wrong14.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 260224 Dec  3 11:36 wrong25.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 102737 Dec  3 11:36 correct50.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 112934 Dec  3 11:36 wrong8.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 238697 Dec  3 11:36 wrong40.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 131062 Dec  3 11:36 wrong43.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  91407 Dec  3 11:36 wrong26.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  95939 Dec  3 11:36 wrong37.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  64215 Dec  3 11:36 correct36.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  86875 Dec  3 11:36 correct3.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  67614 Dec  3 11:36 wrong34.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 136727 Dec  3 11:36 wrong4.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 142392 Dec  3 11:36 wrong11.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 178648 Dec  3 11:36 wrong12.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 136727 Dec  3 11:36 wrong5.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 120865 Dec  3 11:37 wrong47.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 142392 Dec  3 11:37 wrong2.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 110668 Dec  3 11:37 correct20.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 172983 Dec  3 11:37 correct24.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  99338 Dec  3 11:37 correct47.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 117466 Dec  3 11:37 wrong17.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  69880 Dec  3 11:37 wrong31.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 111801 Dec  3 11:37 wrong44.mp4_preprocessed.npy\n-rw-r--r-- 1 root root 165052 Dec  3 11:37 correct27.mp4_preprocessed.npy\n-rw-r--r-- 1 root root  77811 Dec  3 11:37 correct31.mp4_preprocessed.npy\ndrwxr-xr-x 2 root root   4096 Dec  3 11:37 .\n-rw-r--r-- 1 root root  73279 Dec  3 11:37 wrong32.mp4_preprocessed.npy\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import numpy as np\n\n# Specify the path to your .npy file\nfile_path = '/kaggle/working/preprocessed_data/wrong32.mp4_preprocessed.npy'\n\n# Load the data from the .npy file\ndata = np.load(file_path, allow_pickle=True)\n\n# Print the type of data to understand its structure\nprint(type(data))\nprint(data.shape)\nprint(len(data))\n# If it's a list of dictionaries, you can inspect individual elements\nprint(data[0])  # Print the first element to see its structure\n\n# Optionally, print more elements or specific keys if needed\nfor item in data[:5]:  # View the first 5 entries\n    print(item)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T11:52:26.138641Z","iopub.execute_input":"2024-12-03T11:52:26.139390Z","iopub.status.idle":"2024-12-03T11:52:26.149276Z","shell.execute_reply.started":"2024-12-03T11:52:26.139351Z","shell.execute_reply":"2024-12-03T11:52:26.148315Z"}},"outputs":[{"name":"stdout","text":"<class 'numpy.ndarray'>\n(65,)\n65\n{'frame': 1, 'left_arm_angle': 168.8868973843826, 'right_arm_angle': 169.29243989846563, 'landmarks': [(0.8566663861274719, 0.37731680274009705, -0.09211250394582748), (0.866101861000061, 0.35935208201408386, -0.07533276826143265), (0.8658832311630249, 0.3563706576824188, -0.07546903938055038), (0.8658827543258667, 0.3526790738105774, -0.07558848708868027), (0.8654646873474121, 0.35698917508125305, -0.12583020329475403), (0.8646005392074585, 0.3527193069458008, -0.1258518546819687), (0.8639950752258301, 0.3477899134159088, -0.12584808468818665), (0.8498237729072571, 0.31134480237960815, 0.0257448460906744), (0.8488703966140747, 0.30833902955055237, -0.20123735070228577), (0.837600827217102, 0.3745482265949249, -0.04886886849999428), (0.8362727165222168, 0.36985698342323303, -0.11491266638040543), (0.7276058197021484, 0.33265307545661926, 0.15302757918834686), (0.7189415097236633, 0.33552035689353943, -0.27987566590309143), (0.698556661605835, 0.5781166553497314, 0.16779103875160217), (0.6913226842880249, 0.605066180229187, -0.34626829624176025), (0.7140844464302063, 0.7815951704978943, 0.062256235629320145), (0.7115643620491028, 0.8432655334472656, -0.32548290491104126), (0.7418248653411865, 0.8020545840263367, 0.05577246472239494), (0.735878050327301, 0.8648771643638611, -0.3912883698940277), (0.7433498501777649, 0.8080177307128906, 0.007891375571489334), (0.7421424984931946, 0.8560935258865356, -0.3473958373069763), (0.7330660223960876, 0.807720422744751, 0.03786398097872734), (0.7334611415863037, 0.8504747748374939, -0.3116803765296936), (0.49547886848449707, 0.3779752850532532, 0.13029645383358002), (0.4829367995262146, 0.40710893273353577, -0.12996616959571838), (0.3450334966182709, 0.5570377111434937, 0.17920352518558502), (0.33688610792160034, 0.5665866732597351, -0.07268895208835602), (0.18513265252113342, 0.6395429372787476, 0.2919526696205139), (0.17459538578987122, 0.6547060012817383, 0.032587796449661255), (0.15725108981132507, 0.6290932297706604, 0.3002447485923767), (0.1423036754131317, 0.6466482281684875, 0.03846187889575958), (0.1731087863445282, 0.7411825656890869, 0.2284800112247467), (0.17362958192825317, 0.7619667053222656, -0.046587780117988586)]}\n{'frame': 1, 'left_arm_angle': 168.8868973843826, 'right_arm_angle': 169.29243989846563, 'landmarks': [(0.8566663861274719, 0.37731680274009705, -0.09211250394582748), (0.866101861000061, 0.35935208201408386, -0.07533276826143265), (0.8658832311630249, 0.3563706576824188, -0.07546903938055038), (0.8658827543258667, 0.3526790738105774, -0.07558848708868027), (0.8654646873474121, 0.35698917508125305, -0.12583020329475403), (0.8646005392074585, 0.3527193069458008, -0.1258518546819687), (0.8639950752258301, 0.3477899134159088, -0.12584808468818665), (0.8498237729072571, 0.31134480237960815, 0.0257448460906744), (0.8488703966140747, 0.30833902955055237, -0.20123735070228577), (0.837600827217102, 0.3745482265949249, -0.04886886849999428), (0.8362727165222168, 0.36985698342323303, -0.11491266638040543), (0.7276058197021484, 0.33265307545661926, 0.15302757918834686), (0.7189415097236633, 0.33552035689353943, -0.27987566590309143), (0.698556661605835, 0.5781166553497314, 0.16779103875160217), (0.6913226842880249, 0.605066180229187, -0.34626829624176025), (0.7140844464302063, 0.7815951704978943, 0.062256235629320145), (0.7115643620491028, 0.8432655334472656, -0.32548290491104126), (0.7418248653411865, 0.8020545840263367, 0.05577246472239494), (0.735878050327301, 0.8648771643638611, -0.3912883698940277), (0.7433498501777649, 0.8080177307128906, 0.007891375571489334), (0.7421424984931946, 0.8560935258865356, -0.3473958373069763), (0.7330660223960876, 0.807720422744751, 0.03786398097872734), (0.7334611415863037, 0.8504747748374939, -0.3116803765296936), (0.49547886848449707, 0.3779752850532532, 0.13029645383358002), (0.4829367995262146, 0.40710893273353577, -0.12996616959571838), (0.3450334966182709, 0.5570377111434937, 0.17920352518558502), (0.33688610792160034, 0.5665866732597351, -0.07268895208835602), (0.18513265252113342, 0.6395429372787476, 0.2919526696205139), (0.17459538578987122, 0.6547060012817383, 0.032587796449661255), (0.15725108981132507, 0.6290932297706604, 0.3002447485923767), (0.1423036754131317, 0.6466482281684875, 0.03846187889575958), (0.1731087863445282, 0.7411825656890869, 0.2284800112247467), (0.17362958192825317, 0.7619667053222656, -0.046587780117988586)]}\n{'frame': 2, 'left_arm_angle': 169.5352682674291, 'right_arm_angle': 169.19807501010487, 'landmarks': [(0.8561466932296753, 0.3774592876434326, -0.08858256042003632), (0.8661041259765625, 0.3590027987957001, -0.0751381665468216), (0.865872859954834, 0.35581567883491516, -0.07527154684066772), (0.8658286929130554, 0.35182610154151917, -0.07538555562496185), (0.8654628992080688, 0.3564055562019348, -0.12396705150604248), (0.8645790219306946, 0.35170942544937134, -0.12398630380630493), (0.8639264106750488, 0.3461833894252777, -0.12398411333560944), (0.8497446775436401, 0.309802770614624, 0.01780906692147255), (0.8485886454582214, 0.3075115978717804, -0.19909676909446716), (0.835084080696106, 0.3744153380393982, -0.047519437968730927), (0.834559440612793, 0.3695049285888672, -0.10981724411249161), (0.726180911064148, 0.3320206105709076, 0.15552213788032532), (0.7189348936080933, 0.3355175256729126, -0.2798525393009186), (0.6968168020248413, 0.5746601819992065, 0.19835303723812103), (0.6907497644424438, 0.6058375835418701, -0.3131599724292755), (0.7092567682266235, 0.7743687033653259, 0.12754447758197784), (0.711280882358551, 0.8478314876556396, -0.268849641084671), (0.7355033755302429, 0.7897651791572571, 0.12347382307052612), (0.736318051815033, 0.8720108270645142, -0.32498782873153687), (0.7382590174674988, 0.7994908094406128, 0.07783180475234985), (0.7428706288337708, 0.8602175116539001, -0.2914869487285614), (0.7291918396949768, 0.8019334673881531, 0.1049371138215065), (0.733899712562561, 0.8528414964675903, -0.25749924778938293), (0.4855777323246002, 0.4320942759513855, 0.12830010056495667), (0.4670754075050354, 0.44522684812545776, -0.1280377209186554), (0.3138759732246399, 0.5925767421722412, 0.17971684038639069), (0.2785335183143616, 0.6275799870491028, -0.07272638380527496), (0.12343714386224747, 0.6377544403076172, 0.29186734557151794), (0.08708397299051285, 0.646318793296814, 0.01850959286093712), (0.08728896081447601, 0.6115504503250122, 0.2998780906200409), (0.07049120217561722, 0.6165841221809387, 0.022029653191566467), (0.11896486580371857, 0.7447152733802795, 0.2345850169658661), (0.1007852554321289, 0.7632420063018799, -0.06775876134634018)]}\n{'frame': 3, 'left_arm_angle': 169.20242400002462, 'right_arm_angle': 169.11049095465452, 'landmarks': [(0.8562127351760864, 0.3791371285915375, -0.08550772815942764), (0.8664095997810364, 0.3591640889644623, -0.07367859780788422), (0.866020679473877, 0.3558536171913147, -0.07380397617816925), (0.8658703565597534, 0.3518187701702118, -0.07389380037784576), (0.8657716512680054, 0.35647842288017273, -0.12342190742492676), (0.8647807836532593, 0.3517000079154968, -0.1234394907951355), (0.8640487790107727, 0.3459874093532562, -0.12344396114349365), (0.849733829498291, 0.3091115653514862, 0.018396764993667603), (0.8485468626022339, 0.3069285750389099, -0.20259492099285126), (0.8346983194351196, 0.37462082505226135, -0.043561629951000214), (0.8343605995178223, 0.3696609139442444, -0.1080518513917923), (0.7258804440498352, 0.328948438167572, 0.1734796017408371), (0.7190034985542297, 0.33443766832351685, -0.29058215022087097), (0.6957387924194336, 0.5678933262825012, 0.22788023948669434), (0.6902095079421997, 0.6041203141212463, -0.3195868134498596), (0.708293616771698, 0.7670036554336548, 0.14567318558692932), (0.7110220193862915, 0.8522226810455322, -0.27918168902397156), (0.7347040176391602, 0.7822275161743164, 0.14236794412136078), (0.7365992069244385, 0.8768681883811951, -0.34358686208724976), (0.7374221086502075, 0.793080747127533, 0.09310932457447052), (0.7432217597961426, 0.8626598119735718, -0.29895466566085815), (0.7281774878501892, 0.7957900762557983, 0.12144418805837631), (0.7340428829193115, 0.8546551465988159, -0.2627106308937073), (0.4814697802066803, 0.44648680090904236, 0.13213545083999634), (0.4634942412376404, 0.46212148666381836, -0.1319943368434906), (0.3004459738731384, 0.5899797081947327, 0.19765038788318634), (0.2681913375854492, 0.628817081451416, -0.07280472666025162), (0.1107059121131897, 0.6289216876029968, 0.3169221580028534), (0.07078397274017334, 0.6416957378387451, 0.020101837813854218), (0.06723661720752716, 0.6085020899772644, 0.32545098662376404), (0.048031505197286606, 0.6119259595870972, 0.02371254749596119), (0.12199775874614716, 0.753369927406311, 0.2684449255466461), (0.08363188803195953, 0.7741345167160034, -0.07177634537220001)]}\n{'frame': 4, 'left_arm_angle': 168.9387013352447, 'right_arm_angle': 168.95452809616904, 'landmarks': [(0.8541997671127319, 0.3811037242412567, -0.07595652341842651), (0.8656193614006042, 0.359566330909729, -0.06571274250745773), (0.865175724029541, 0.35608139634132385, -0.0658227875828743), (0.8649266362190247, 0.35189467668533325, -0.06589715927839279), (0.8650985956192017, 0.3567948341369629, -0.11619842052459717), (0.8642086982727051, 0.35180214047431946, -0.11622019857168198), (0.8635508418083191, 0.3459899127483368, -0.1162475124001503), (0.8490206003189087, 0.30906087160110474, 0.02206289954483509), (0.848138689994812, 0.30683717131614685, -0.2014506608247757), (0.8321253657341003, 0.3755333423614502, -0.03433919697999954), (0.8318384289741516, 0.3706384301185608, -0.09978573769330978), (0.7239729166030884, 0.32727670669555664, 0.1853373497724533), (0.7194371819496155, 0.3335166573524475, -0.29142147302627563), (0.6932159662246704, 0.5643079280853271, 0.23910100758075714), (0.6894937753677368, 0.6039250493049622, -0.3245299458503723), (0.7059489488601685, 0.7629330158233643, 0.15434566140174866), (0.7102360129356384, 0.8547902703285217, -0.2914038598537445), (0.7339714765548706, 0.7750388383865356, 0.15078648924827576), (0.7378590703010559, 0.8822429180145264, -0.35290318727493286), (0.7366188764572144, 0.7876688838005066, 0.10417498648166656), (0.7441431879997253, 0.8649928569793701, -0.3130016624927521), (0.7262512445449829, 0.7907711267471313, 0.13067708909511566), (0.7341538667678833, 0.8560967445373535, -0.27706095576286316), (0.4768448770046234, 0.46164143085479736, 0.1332620084285736), (0.4607742726802826, 0.47596168518066406, -0.1331627070903778), (0.28921014070510864, 0.6115881204605103, 0.2123103141784668), (0.2626931965351105, 0.6351191401481628, -0.06192454695701599), (0.10759619623422623, 0.6291719079017639, 0.3564610481262207), (0.06449271738529205, 0.6380860805511475, 0.05190052464604378), (0.06403994560241699, 0.6078183650970459, 0.36800092458724976), (0.04093427211046219, 0.6121696829795837, 0.05793933942914009), (0.1056831106543541, 0.7542369365692139, 0.31385040283203125), (0.0721568763256073, 0.7795572280883789, -0.037511445581912994)]}\n{'frame': 5, 'left_arm_angle': 168.61202806806313, 'right_arm_angle': 168.71277613431303, 'landmarks': [(0.8542172312736511, 0.3848962187767029, -0.0758102759718895), (0.8657013177871704, 0.3609817624092102, -0.06492693722248077), (0.8652082681655884, 0.3572891354560852, -0.065033458173275), (0.8649312853813171, 0.3529362082481384, -0.06510528177022934), (0.8652051091194153, 0.3581881523132324, -0.11504793167114258), (0.8642984628677368, 0.35291367769241333, -0.11507292836904526), (0.8636153936386108, 0.34676581621170044, -0.11509623378515244), (0.8489203453063965, 0.3103821873664856, 0.023458173498511314), (0.8481407165527344, 0.3071284890174866, -0.19888880848884583), (0.8322890400886536, 0.3791324496269226, -0.03374157473444939), (0.8318803906440735, 0.37477797269821167, -0.09863018989562988), (0.7238246202468872, 0.32818174362182617, 0.18361005187034607), (0.7194796800613403, 0.33302435278892517, -0.2913345694541931), (0.6923118829727173, 0.5621315240859985, 0.23634609580039978), (0.6889432668685913, 0.6012029051780701, -0.32557055354118347), (0.705288290977478, 0.7619041204452515, 0.14542582631111145), (0.7102483510971069, 0.8553885817527771, -0.2950204908847809), (0.7342860698699951, 0.7752326726913452, 0.13973000645637512), (0.7407975196838379, 0.8838830590248108, -0.35888350009918213), (0.7368271946907043, 0.786431610584259, 0.09325151890516281), (0.7460054159164429, 0.8655062317848206, -0.3189767897129059), (0.7259624600410461, 0.7884557843208313, 0.12186013162136078), (0.7346689701080322, 0.8562214970588684, -0.27827951312065125), (0.47744855284690857, 0.4708540141582489, 0.1332612931728363), (0.45977339148521423, 0.48071688413619995, -0.13316738605499268), (0.2911272644996643, 0.6137679219245911, 0.21257469058036804), (0.2619536221027374, 0.639658510684967, -0.055983591824769974), (0.11408597230911255, 0.6290895938873291, 0.3541523814201355), (0.06900686025619507, 0.6387830376625061, 0.057589806616306305), (0.07963889837265015, 0.6002975702285767, 0.3651600182056427), (0.04080049693584442, 0.6137255430221558, 0.06299625337123871), (0.12066507339477539, 0.7506972551345825, 0.31379708647727966), (0.08118848502635956, 0.7790473103523254, -0.0312399473041296)]}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 3d-Resnet","metadata":{}},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"input_folder = '/kaggle/working/preprocessed_data'\nsequence_length = 100\nbatch_size = 16\nnum_epochs = 50\nlearning_rate = 0.001\nweight_decay = 1e-5\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:10:25.586144Z","iopub.execute_input":"2024-12-03T13:10:25.586847Z","iopub.status.idle":"2024-12-03T13:10:25.591665Z","shell.execute_reply.started":"2024-12-03T13:10:25.586815Z","shell.execute_reply":"2024-12-03T13:10:25.590687Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, Dataset\n\n\n# Dataset Class\nclass PushupDataset(Dataset):\n    def __init__(self, data: List[np.ndarray], labels: List[int], sequence_length: int = 100):\n        \"\"\"\n        Initialize the PushupDataset.\n\n        Args:\n            data: List of numpy arrays containing sequence data\n            labels: List of labels (0 for incorrect, 1 for correct)\n            sequence_length: Target sequence length for padding/truncating\n        \"\"\"\n        self.data = data\n        self.labels = labels\n        self.sequence_length = sequence_length\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        sequence = self.data[idx]\n        label = self.labels[idx]\n\n        # Pad or truncate sequence\n        if len(sequence) < self.sequence_length:\n            pad_length = self.sequence_length - len(sequence)\n            sequence = np.pad(sequence, ((0, pad_length), (0, 0)), mode='constant')\n        elif len(sequence) > self.sequence_length:\n            sequence = sequence[:self.sequence_length]\n\n        # Permute the sequence dimensions to match the model's expected input\n        sequence = torch.FloatTensor(sequence).permute(1, 0)  # Shape: [num_features, sequence_length]\n        return sequence, torch.LongTensor([label])\n\n\n# Model Classes\nclass Conv3DBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,\n                 stride: int = 1, padding: int = 1):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU(inplace=False)  # Ensure inplace=False\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.relu(self.bn(self.conv(x)))\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.conv1 = Conv3DBlock(in_channels, out_channels)\n        self.conv2 = Conv3DBlock(out_channels, out_channels)\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1),\n                nn.BatchNorm3d(out_channels)\n            )\n        self.relu = nn.ReLU(inplace=False)  # Ensure inplace=False\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out = out + identity  # Change from in-place addition\n        return self.relu(out)\n\n\nclass ResNet3D(nn.Module):\n    def __init__(self, block: nn.Module, layers: List[int], num_classes: int = 2,\n                 input_channels: int = 8):\n        super().__init__()\n        self.in_channels = 64\n        self.conv1 = Conv3DBlock(input_channels, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block: nn.Module, out_channels: int, blocks: int,\n                    stride: int = 1) -> nn.Sequential:\n        layers = []\n        layers.append(block(self.in_channels, out_channels))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x shape: [batch_size, channels, sequence_length]\n        x = x.unsqueeze(2).unsqueeze(-1)  # Shape: [batch_size, channels, 1, sequence_length, 1]\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\ndef resnet3d18(num_classes: int = 2, input_channels: int = 8) -> ResNet3D:\n    return ResNet3D(ResidualBlock, [2, 2, 2, 2], num_classes, input_channels)\n\n\n# Training Function\ndef train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n                num_epochs: int, learning_rate: float, weight_decay: float,\n                device: torch.device = None) -> nn.Module:\n    \"\"\"\n    Train the model.\n\n    Args:\n        model: The ResNet3D model to train\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        num_epochs: Number of epochs to train\n        learning_rate: Initial learning rate\n        weight_decay: Weight decay for optimizer\n        device: Device to train on (default: None, will use cuda if available)\n\n    Returns:\n        Trained model\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\", flush=True)\n\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n\n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        for batch_data, batch_labels in train_loader:\n            batch_data = batch_data.to(device)\n            batch_labels = batch_labels.squeeze().to(device)\n\n            optimizer.zero_grad()\n            outputs = model(batch_data)\n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_total += batch_labels.size(0)\n            train_correct += predicted.eq(batch_labels).sum().item()\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for batch_data, batch_labels in val_loader:\n                batch_data = batch_data.to(device)\n                batch_labels = batch_labels.squeeze().to(device)\n\n                outputs = model(batch_data)\n                loss = criterion(outputs, batch_labels)\n                val_loss += loss.item()\n\n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n\n        # Calculate metrics\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        train_accuracy = 100. * train_correct / train_total\n        val_accuracy = 100. * val_correct / val_total\n\n        print(f'Epoch [{epoch+1}/{num_epochs}]', flush=True)\n        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%', flush=True)\n        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%', flush=True)\n\n        # Learning rate scheduling\n        scheduler.step(val_loss)\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, 'best_model.pth')\n            print(\"Saved new best model\", flush=True)\n\n        print('-' * 60, flush=True)\n\n    print('Training completed.', flush=True)\n    return model\n\n\n# Feature Extraction Function\ndef extract_features(frame: dict) -> List[float]:\n    \"\"\"Extract features from a single frame\"\"\"\n    return [\n        frame['left_arm_angle'],\n        frame['right_arm_angle'],\n        frame['landmarks'][11][1],  # LEFT_SHOULDER y-coordinate\n        frame['landmarks'][12][1],  # RIGHT_SHOULDER y-coordinate\n        frame['landmarks'][13][1],  # LEFT_ELBOW y-coordinate\n        frame['landmarks'][14][1],  # RIGHT_ELBOW y-coordinate\n        frame['landmarks'][15][1],  # LEFT_WRIST y-coordinate\n        frame['landmarks'][16][1]   # RIGHT_WRIST y-coordinate\n    ]\n\n\n# Data Preparation Function\ndef prepare_data(input_folder: str, sequence_length: int = 100,\n                 batch_size: int = 16) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Prepare data for training and validation with progress tracking and optimizations.\n    \"\"\"\n    start_time = time.time()\n    all_data = []\n    all_labels = []\n    total_files = len([f for f in os.listdir(input_folder) if f.endswith('_preprocessed.npy')])\n    processed_files = 0\n\n    print(f\"Starting to process {total_files} files...\", flush=True)\n\n    for file in os.listdir(input_folder):\n        if file.endswith('_preprocessed.npy'):\n            try:\n                print(f\"Processing file {processed_files + 1}/{total_files}: {file}\", flush=True)\n                file_start_time = time.time()\n\n                # Load data\n                data = np.load(os.path.join(input_folder, file), allow_pickle=True)\n\n                # Pre-allocate features array for better performance\n                features = np.zeros((len(data), 8))\n\n                # Extract features\n                for i, frame in enumerate(data):\n                    features[i] = extract_features(frame)\n\n                all_data.append(features)\n                label = 1 if \"correct\" in file.lower() else 0\n                all_labels.append(label)\n\n                processed_files += 1\n                file_time = time.time() - file_start_time\n                print(f\"Processed {file} in {file_time:.2f} seconds\", flush=True)\n\n            except Exception as e:\n                print(f\"Error processing file {file}: {str(e)}\", flush=True)\n                continue\n\n    if not all_data:\n        raise ValueError(\"No valid data files found in the input folder\")\n\n    print(\"\\nNormalizing data...\", flush=True)\n\n    # Optimize normalization by processing all sequences at once\n    all_data_flat = np.vstack([np.array(seq) for seq in all_data])\n    scaler = StandardScaler()\n    all_data_normalized_flat = scaler.fit_transform(all_data_flat)\n\n    # Reshape back to original structure\n    cumulative_lengths = np.cumsum([len(seq) for seq in all_data])\n    all_data_normalized = np.split(all_data_normalized_flat, cumulative_lengths[:-1])\n\n    # Standardize sequence lengths\n    all_data_normalized = [\n        seq[:sequence_length] if len(seq) > sequence_length\n        else np.pad(seq, ((0, sequence_length - len(seq)), (0, 0)), mode='constant')\n        for seq in all_data_normalized\n    ]\n\n    print(\"Splitting data into train and validation sets...\", flush=True)\n    X_train, X_val, y_train, y_val = train_test_split(\n        all_data_normalized, all_labels, test_size=0.2, random_state=42\n    )\n\n    # Create datasets and data loaders\n    train_dataset = PushupDataset(X_train, y_train, sequence_length)\n    val_dataset = PushupDataset(X_val, y_val, sequence_length)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,  # Set to 0 to avoid issues with multiprocessing\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True\n    )\n\n    total_time = time.time() - start_time\n    print(f\"\\nData preparation completed in {total_time:.2f} seconds!\")\n    print(f\"Number of training batches: {len(train_loader)}\")\n    print(f\"Number of validation batches: {len(val_loader)}\")\n\n    return train_loader, val_loader\n\n\n# Main Execution\nif __name__ == \"__main__\":\n    # Hyperparameters\n    input_folder = \"/kaggle/working/preprocessed_data\"  # Replace with your data folder path\n    sequence_length = 100\n    batch_size = 16\n    num_epochs = 25\n    learning_rate = 0.001\n    weight_decay = 1e-4\n\n    # Prepare data\n    train_loader, val_loader = prepare_data(\n        input_folder=input_folder,\n        sequence_length=sequence_length,\n        batch_size=batch_size\n    )\n\n    # Initialize model\n    model = resnet3d18(num_classes=2, input_channels=8)\n\n    # Start training\n    print(\"Starting training...\", flush=True)\n    trained_model = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay\n    )\n    print(\"Training completed.\", flush=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:28:21.116889Z","iopub.execute_input":"2024-12-03T13:28:21.117306Z","iopub.status.idle":"2024-12-03T13:28:31.709700Z","shell.execute_reply.started":"2024-12-03T13:28:21.117271Z","shell.execute_reply":"2024-12-03T13:28:31.708628Z"}},"outputs":[{"name":"stdout","text":"Starting to process 100 files...\nProcessing file 1/100: wrong11.mp4_preprocessed.npy\nProcessed wrong11.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 2/100: wrong24.mp4_preprocessed.npy\nProcessed wrong24.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 3/100: wrong49.mp4_preprocessed.npy\nProcessed wrong49.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 4/100: wrong2.mp4_preprocessed.npy\nProcessed wrong2.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 5/100: wrong47.mp4_preprocessed.npy\nProcessed wrong47.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 6/100: correct18.mp4_preprocessed.npy\nProcessed correct18.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 7/100: wrong5.mp4_preprocessed.npy\nProcessed wrong5.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 8/100: correct20.mp4_preprocessed.npy\nProcessed correct20.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 9/100: correct41.mp4_preprocessed.npy\nProcessed correct41.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 10/100: wrong7.mp4_preprocessed.npy\nProcessed wrong7.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 11/100: correct48.mp4_preprocessed.npy\nProcessed correct48.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 12/100: correct22.mp4_preprocessed.npy\nProcessed correct22.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 13/100: correct12.mp4_preprocessed.npy\nProcessed correct12.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 14/100: correct2.mp4_preprocessed.npy\nProcessed correct2.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 15/100: wrong48.mp4_preprocessed.npy\nProcessed wrong48.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 16/100: wrong40.mp4_preprocessed.npy\nProcessed wrong40.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 17/100: wrong34.mp4_preprocessed.npy\nProcessed wrong34.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 18/100: wrong35.mp4_preprocessed.npy\nProcessed wrong35.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 19/100: wrong9.mp4_preprocessed.npy\nProcessed wrong9.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 20/100: correct40.mp4_preprocessed.npy\nProcessed correct40.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 21/100: wrong31.mp4_preprocessed.npy\nProcessed wrong31.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 22/100: correct31.mp4_preprocessed.npy\nProcessed correct31.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 23/100: correct5.mp4_preprocessed.npy\nProcessed correct5.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 24/100: wrong17.mp4_preprocessed.npy\nProcessed wrong17.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 25/100: wrong33.mp4_preprocessed.npy\nProcessed wrong33.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 26/100: wrong16.mp4_preprocessed.npy\nProcessed wrong16.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 27/100: correct9.mp4_preprocessed.npy\nProcessed correct9.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 28/100: correct1.mp4_preprocessed.npy\nProcessed correct1.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 29/100: correct49.mp4_preprocessed.npy\nProcessed correct49.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 30/100: correct36.mp4_preprocessed.npy\nProcessed correct36.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 31/100: correct14.mp4_preprocessed.npy\nProcessed correct14.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 32/100: correct17.mp4_preprocessed.npy\nProcessed correct17.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 33/100: correct24.mp4_preprocessed.npy\nProcessed correct24.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 34/100: wrong20.mp4_preprocessed.npy\nProcessed wrong20.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 35/100: wrong6.mp4_preprocessed.npy\nProcessed wrong6.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 36/100: wrong45.mp4_preprocessed.npy\nProcessed wrong45.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 37/100: wrong27.mp4_preprocessed.npy\nProcessed wrong27.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 38/100: correct47.mp4_preprocessed.npy\nProcessed correct47.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 39/100: wrong8.mp4_preprocessed.npy\nProcessed wrong8.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 40/100: correct28.mp4_preprocessed.npy\nProcessed correct28.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 41/100: wrong29.mp4_preprocessed.npy\nProcessed wrong29.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 42/100: wrong32.mp4_preprocessed.npy\nProcessed wrong32.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 43/100: correct7.mp4_preprocessed.npy\nProcessed correct7.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 44/100: wrong28.mp4_preprocessed.npy\nProcessed wrong28.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 45/100: correct50.mp4_preprocessed.npy\nProcessed correct50.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 46/100: wrong23.mp4_preprocessed.npy\nProcessed wrong23.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 47/100: wrong38.mp4_preprocessed.npy\nProcessed wrong38.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 48/100: correct44.mp4_preprocessed.npy\nProcessed correct44.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 49/100: wrong3.mp4_preprocessed.npy\nProcessed wrong3.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 50/100: wrong15.mp4_preprocessed.npy\nProcessed wrong15.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 51/100: correct46.mp4_preprocessed.npy\nProcessed correct46.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 52/100: correct27.mp4_preprocessed.npy\nProcessed correct27.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 53/100: correct3.mp4_preprocessed.npy\nProcessed correct3.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 54/100: correct32.mp4_preprocessed.npy\nProcessed correct32.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 55/100: wrong19.mp4_preprocessed.npy\nProcessed wrong19.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 56/100: correct29.mp4_preprocessed.npy\nProcessed correct29.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 57/100: correct34.mp4_preprocessed.npy\nProcessed correct34.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 58/100: correct6.mp4_preprocessed.npy\nProcessed correct6.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 59/100: wrong26.mp4_preprocessed.npy\nProcessed wrong26.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 60/100: wrong12.mp4_preprocessed.npy\nProcessed wrong12.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 61/100: correct11.mp4_preprocessed.npy\nProcessed correct11.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 62/100: correct25.mp4_preprocessed.npy\nProcessed correct25.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 63/100: correct13.mp4_preprocessed.npy\nProcessed correct13.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 64/100: correct10.mp4_preprocessed.npy\nProcessed correct10.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 65/100: correct19.mp4_preprocessed.npy\nProcessed correct19.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 66/100: correct16.mp4_preprocessed.npy\nProcessed correct16.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 67/100: wrong13.mp4_preprocessed.npy\nProcessed wrong13.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 68/100: correct23.mp4_preprocessed.npy\nProcessed correct23.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 69/100: correct39.mp4_preprocessed.npy\nProcessed correct39.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 70/100: correct42.mp4_preprocessed.npy\nProcessed correct42.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 71/100: wrong46.mp4_preprocessed.npy\nProcessed wrong46.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 72/100: wrong25.mp4_preprocessed.npy\nProcessed wrong25.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 73/100: correct35.mp4_preprocessed.npy\nProcessed correct35.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 74/100: wrong39.mp4_preprocessed.npy\nProcessed wrong39.mp4_preprocessed.npy in 0.01 seconds\nProcessing file 75/100: correct43.mp4_preprocessed.npy\nProcessed correct43.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 76/100: correct33.mp4_preprocessed.npy\nProcessed correct33.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 77/100: wrong22.mp4_preprocessed.npy\nProcessed wrong22.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 78/100: wrong4.mp4_preprocessed.npy\nProcessed wrong4.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 79/100: wrong44.mp4_preprocessed.npy\nProcessed wrong44.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 80/100: correct38.mp4_preprocessed.npy\nProcessed correct38.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 81/100: correct4.mp4_preprocessed.npy\nProcessed correct4.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 82/100: wrong43.mp4_preprocessed.npy\nProcessed wrong43.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 83/100: wrong50.mp4_preprocessed.npy\nProcessed wrong50.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 84/100: wrong36.mp4_preprocessed.npy\nProcessed wrong36.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 85/100: correct8.mp4_preprocessed.npy\nProcessed correct8.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 86/100: wrong21.mp4_preprocessed.npy\nProcessed wrong21.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 87/100: wrong14.mp4_preprocessed.npy\nProcessed wrong14.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 88/100: wrong37.mp4_preprocessed.npy\nProcessed wrong37.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 89/100: wrong41.mp4_preprocessed.npy\nProcessed wrong41.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 90/100: correct45.mp4_preprocessed.npy\nProcessed correct45.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 91/100: wrong18.mp4_preprocessed.npy\nProcessed wrong18.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 92/100: correct26.mp4_preprocessed.npy\nProcessed correct26.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 93/100: correct37.mp4_preprocessed.npy\nProcessed correct37.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 94/100: wrong30.mp4_preprocessed.npy\nProcessed wrong30.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 95/100: wrong42.mp4_preprocessed.npy\nProcessed wrong42.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 96/100: correct21.mp4_preprocessed.npy\nProcessed correct21.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 97/100: wrong10.mp4_preprocessed.npy\nProcessed wrong10.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 98/100: correct30.mp4_preprocessed.npy\nProcessed correct30.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 99/100: wrong1.mp4_preprocessed.npy\nProcessed wrong1.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 100/100: correct15.mp4_preprocessed.npy\nProcessed correct15.mp4_preprocessed.npy in 0.00 seconds\n\nNormalizing data...\nSplitting data into train and validation sets...\n\nData preparation completed in 0.33 seconds!\nNumber of training batches: 5\nNumber of validation batches: 2\nStarting training...\nUsing device: cuda\nEpoch [1/25]\nTrain Loss: 0.8888, Train Accuracy: 72.50%\nVal Loss: 0.6925, Val Accuracy: 50.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [2/25]\nTrain Loss: 0.6645, Train Accuracy: 80.00%\nVal Loss: 0.9462, Val Accuracy: 55.00%\n------------------------------------------------------------\nEpoch [3/25]\nTrain Loss: 0.4262, Train Accuracy: 90.00%\nVal Loss: 0.9051, Val Accuracy: 55.00%\n------------------------------------------------------------\nEpoch [4/25]\nTrain Loss: 0.2889, Train Accuracy: 91.25%\nVal Loss: 0.8948, Val Accuracy: 55.00%\n------------------------------------------------------------\nEpoch [5/25]\nTrain Loss: 0.3814, Train Accuracy: 88.75%\nVal Loss: 0.5611, Val Accuracy: 60.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [6/25]\nTrain Loss: 0.2464, Train Accuracy: 92.50%\nVal Loss: 0.5643, Val Accuracy: 80.00%\n------------------------------------------------------------\nEpoch [7/25]\nTrain Loss: 0.1495, Train Accuracy: 97.50%\nVal Loss: 0.7774, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [8/25]\nTrain Loss: 0.3196, Train Accuracy: 88.75%\nVal Loss: 0.2148, Val Accuracy: 90.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [9/25]\nTrain Loss: 0.2628, Train Accuracy: 83.75%\nVal Loss: 0.2364, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [10/25]\nTrain Loss: 0.1385, Train Accuracy: 96.25%\nVal Loss: 0.2348, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [11/25]\nTrain Loss: 0.1201, Train Accuracy: 96.25%\nVal Loss: 0.2430, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [12/25]\nTrain Loss: 0.2810, Train Accuracy: 85.00%\nVal Loss: 0.3337, Val Accuracy: 80.00%\n------------------------------------------------------------\nEpoch [13/25]\nTrain Loss: 0.1148, Train Accuracy: 97.50%\nVal Loss: 0.0852, Val Accuracy: 95.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [14/25]\nTrain Loss: 0.2317, Train Accuracy: 92.50%\nVal Loss: 0.3997, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [15/25]\nTrain Loss: 0.1343, Train Accuracy: 95.00%\nVal Loss: 0.3009, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [16/25]\nTrain Loss: 0.2054, Train Accuracy: 91.25%\nVal Loss: 0.2069, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [17/25]\nTrain Loss: 0.2453, Train Accuracy: 92.50%\nVal Loss: 0.4686, Val Accuracy: 70.00%\n------------------------------------------------------------\nEpoch [18/25]\nTrain Loss: 0.2549, Train Accuracy: 91.25%\nVal Loss: 0.1000, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [19/25]\nTrain Loss: 0.1709, Train Accuracy: 93.75%\nVal Loss: 0.2778, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [20/25]\nTrain Loss: 0.1303, Train Accuracy: 95.00%\nVal Loss: 0.2381, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [21/25]\nTrain Loss: 0.1538, Train Accuracy: 92.50%\nVal Loss: 0.1769, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [22/25]\nTrain Loss: 0.1227, Train Accuracy: 93.75%\nVal Loss: 0.1162, Val Accuracy: 100.00%\n------------------------------------------------------------\nEpoch [23/25]\nTrain Loss: 0.1411, Train Accuracy: 95.00%\nVal Loss: 0.2176, Val Accuracy: 90.00%\n------------------------------------------------------------\nEpoch [24/25]\nTrain Loss: 0.1217, Train Accuracy: 96.25%\nVal Loss: 0.1746, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [25/25]\nTrain Loss: 0.0706, Train Accuracy: 96.25%\nVal Loss: 0.1557, Val Accuracy: 90.00%\n------------------------------------------------------------\nTraining completed.\nTraining completed.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import os\nimport time\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, Dataset\nimport joblib  # Import joblib to save the scaler\n\n\n# Dataset Class\nclass PushupDataset(Dataset):\n    def __init__(self, data: List[np.ndarray], labels: List[int], sequence_length: int = 100):\n        \"\"\"\n        Initialize the PushupDataset.\n\n        Args:\n            data: List of numpy arrays containing sequence data\n            labels: List of labels (0 for incorrect, 1 for correct)\n            sequence_length: Target sequence length for padding/truncating\n        \"\"\"\n        self.data = data\n        self.labels = labels\n        self.sequence_length = sequence_length\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        sequence = self.data[idx]\n        label = self.labels[idx]\n\n        # Pad or truncate sequence\n        if len(sequence) < self.sequence_length:\n            pad_length = self.sequence_length - len(sequence)\n            sequence = np.pad(sequence, ((0, pad_length), (0, 0)), mode='constant')\n        elif len(sequence) > self.sequence_length:\n            sequence = sequence[:self.sequence_length]\n\n        # Permute the sequence dimensions to match the model's expected input\n        sequence = torch.FloatTensor(sequence).permute(1, 0)  # Shape: [num_features, sequence_length]\n        return sequence, torch.LongTensor([label])\n\n\n# Model Classes\nclass Conv3DBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,\n                 stride: int = 1, padding: int = 1):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU(inplace=False)  # Ensure inplace=False\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.relu(self.bn(self.conv(x)))\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n        self.conv1 = Conv3DBlock(in_channels, out_channels)\n        self.conv2 = Conv3DBlock(out_channels, out_channels)\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1),\n                nn.BatchNorm3d(out_channels)\n            )\n        self.relu = nn.ReLU(inplace=False)  # Ensure inplace=False\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        identity = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out = out + identity  # Change from in-place addition\n        return self.relu(out)\n\n\nclass ResNet3D(nn.Module):\n    def __init__(self, block: nn.Module, layers: List[int], num_classes: int = 2,\n                 input_channels: int = 8):\n        super().__init__()\n        self.in_channels = 64\n        self.conv1 = Conv3DBlock(input_channels, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block: nn.Module, out_channels: int, blocks: int,\n                    stride: int = 1) -> nn.Sequential:\n        layers = []\n        layers.append(block(self.in_channels, out_channels))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x shape: [batch_size, channels, sequence_length]\n        x = x.unsqueeze(2).unsqueeze(-1)  # Shape: [batch_size, channels, 1, sequence_length, 1]\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n\ndef resnet3d18(num_classes: int = 2, input_channels: int = 8) -> ResNet3D:\n    return ResNet3D(ResidualBlock, [2, 2, 2, 2], num_classes, input_channels)\n\n\n# Training Function\ndef train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n                num_epochs: int, learning_rate: float, weight_decay: float,\n                device: torch.device = None) -> nn.Module:\n    \"\"\"\n    Train the model.\n\n    Args:\n        model: The ResNet3D model to train\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data\n        num_epochs: Number of epochs to train\n        learning_rate: Initial learning rate\n        weight_decay: Weight decay for optimizer\n        device: Device to train on (default: None, will use cuda if available)\n\n    Returns:\n        Trained model\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\", flush=True)\n\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n\n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n\n        for batch_data, batch_labels in train_loader:\n            batch_data = batch_data.to(device)\n            batch_labels = batch_labels.squeeze().to(device)\n\n            optimizer.zero_grad()\n            outputs = model(batch_data)\n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            train_total += batch_labels.size(0)\n            train_correct += predicted.eq(batch_labels).sum().item()\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n\n        with torch.no_grad():\n            for batch_data, batch_labels in val_loader:\n                batch_data = batch_data.to(device)\n                batch_labels = batch_labels.squeeze().to(device)\n\n                outputs = model(batch_data)\n                loss = criterion(outputs, batch_labels)\n                val_loss += loss.item()\n\n                _, predicted = outputs.max(1)\n                val_total += batch_labels.size(0)\n                val_correct += predicted.eq(batch_labels).sum().item()\n\n        # Calculate metrics\n        train_loss /= len(train_loader)\n        val_loss /= len(val_loader)\n        train_accuracy = 100. * train_correct / train_total\n        val_accuracy = 100. * val_correct / val_total\n\n        print(f'Epoch [{epoch+1}/{num_epochs}]', flush=True)\n        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%', flush=True)\n        print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%', flush=True)\n\n        # Learning rate scheduling\n        scheduler.step(val_loss)\n\n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n            }, 'best_model.pth')\n            print(\"Saved new best model\", flush=True)\n\n        print('-' * 60, flush=True)\n\n    print('Training completed.', flush=True)\n\n    # Save the final trained model\n    torch.save(model.state_dict(), 'final_trained_model.pth')\n    print(\"Final trained model saved as 'final_trained_model.pth'\", flush=True)\n\n    return model\n\n\n# Feature Extraction Function\ndef extract_features(frame: dict) -> List[float]:\n    \"\"\"Extract features from a single frame\"\"\"\n    return [\n        frame['left_arm_angle'],\n        frame['right_arm_angle'],\n        frame['landmarks'][11][1],  # LEFT_SHOULDER y-coordinate\n        frame['landmarks'][12][1],  # RIGHT_SHOULDER y-coordinate\n        frame['landmarks'][13][1],  # LEFT_ELBOW y-coordinate\n        frame['landmarks'][14][1],  # RIGHT_ELBOW y-coordinate\n        frame['landmarks'][15][1],  # LEFT_WRIST y-coordinate\n        frame['landmarks'][16][1]   # RIGHT_WRIST y-coordinate\n    ]\n\n\n# Data Preparation Function\ndef prepare_data(input_folder: str, sequence_length: int = 100,\n                 batch_size: int = 16) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Prepare data for training and validation with progress tracking and optimizations.\n    \"\"\"\n    start_time = time.time()\n    all_data = []\n    all_labels = []\n    total_files = len([f for f in os.listdir(input_folder) if f.endswith('_preprocessed.npy')])\n    processed_files = 0\n\n    print(f\"Starting to process {total_files} files...\", flush=True)\n\n    for file in os.listdir(input_folder):\n        if file.endswith('_preprocessed.npy'):\n            try:\n                print(f\"Processing file {processed_files + 1}/{total_files}: {file}\", flush=True)\n                file_start_time = time.time()\n\n                # Load data\n                data = np.load(os.path.join(input_folder, file), allow_pickle=True)\n\n                # Pre-allocate features array for better performance\n                features = np.zeros((len(data), 8))\n\n                # Extract features\n                for i, frame in enumerate(data):\n                    features[i] = extract_features(frame)\n\n                all_data.append(features)\n                label = 1 if \"correct\" in file.lower() else 0\n                all_labels.append(label)\n\n                processed_files += 1\n                file_time = time.time() - file_start_time\n                print(f\"Processed {file} in {file_time:.2f} seconds\", flush=True)\n\n            except Exception as e:\n                print(f\"Error processing file {file}: {str(e)}\", flush=True)\n                continue\n\n    if not all_data:\n        raise ValueError(\"No valid data files found in the input folder\")\n\n    print(\"\\nNormalizing data...\", flush=True)\n\n    # Optimize normalization by processing all sequences at once\n    all_data_flat = np.vstack([np.array(seq) for seq in all_data])\n    scaler = StandardScaler()\n    all_data_normalized_flat = scaler.fit_transform(all_data_flat)\n\n    # Save the scaler for later use\n    joblib.dump(scaler, 'scaler.pkl')\n    print(\"Scaler saved as 'scaler.pkl'\", flush=True)\n\n    # Reshape back to original structure\n    cumulative_lengths = np.cumsum([len(seq) for seq in all_data])\n    all_data_normalized = np.split(all_data_normalized_flat, cumulative_lengths[:-1])\n\n    # Standardize sequence lengths\n    all_data_normalized = [\n        seq[:sequence_length] if len(seq) > sequence_length\n        else np.pad(seq, ((0, sequence_length - len(seq)), (0, 0)), mode='constant')\n        for seq in all_data_normalized\n    ]\n\n    print(\"Splitting data into train and validation sets...\", flush=True)\n    X_train, X_val, y_train, y_val = train_test_split(\n        all_data_normalized, all_labels, test_size=0.2, random_state=42\n    )\n\n    # Create datasets and data loaders\n    train_dataset = PushupDataset(X_train, y_train, sequence_length)\n    val_dataset = PushupDataset(X_val, y_val, sequence_length)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,  # Set to 0 to avoid issues with multiprocessing\n        pin_memory=True\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True\n    )\n\n    total_time = time.time() - start_time\n    print(f\"\\nData preparation completed in {total_time:.2f} seconds!\")\n    print(f\"Number of training batches: {len(train_loader)}\")\n    print(f\"Number of validation batches: {len(val_loader)}\")\n\n    return train_loader, val_loader\n\n\n# Main Execution\nif __name__ == \"__main__\":\n    # Hyperparameters\n    input_folder = \"/kaggle/working/preprocessed_data\"  # Replace with your data folder path\n    sequence_length = 100\n    batch_size = 16\n    num_epochs = 25\n    learning_rate = 0.001\n    weight_decay = 1e-4\n\n    # Prepare data\n    train_loader, val_loader = prepare_data(\n        input_folder=input_folder,\n        sequence_length=sequence_length,\n        batch_size=batch_size\n    )\n\n    # Initialize model\n    model = resnet3d18(num_classes=2, input_channels=8)\n\n    # Start training\n    print(\"Starting training...\", flush=True)\n    trained_model = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay\n    )\n    print(\"Training completed.\", flush=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:41:23.434761Z","iopub.execute_input":"2024-12-03T13:41:23.435313Z","iopub.status.idle":"2024-12-03T13:41:39.622284Z","shell.execute_reply.started":"2024-12-03T13:41:23.435277Z","shell.execute_reply":"2024-12-03T13:41:39.621340Z"}},"outputs":[{"name":"stdout","text":"Starting to process 100 files...\nProcessing file 1/100: wrong11.mp4_preprocessed.npy\nProcessed wrong11.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 2/100: wrong24.mp4_preprocessed.npy\nProcessed wrong24.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 3/100: wrong49.mp4_preprocessed.npy\nProcessed wrong49.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 4/100: wrong2.mp4_preprocessed.npy\nProcessed wrong2.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 5/100: wrong47.mp4_preprocessed.npy\nProcessed wrong47.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 6/100: correct18.mp4_preprocessed.npy\nProcessed correct18.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 7/100: wrong5.mp4_preprocessed.npy\nProcessed wrong5.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 8/100: correct20.mp4_preprocessed.npy\nProcessed correct20.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 9/100: correct41.mp4_preprocessed.npy\nProcessed correct41.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 10/100: wrong7.mp4_preprocessed.npy\nProcessed wrong7.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 11/100: correct48.mp4_preprocessed.npy\nProcessed correct48.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 12/100: correct22.mp4_preprocessed.npy\nProcessed correct22.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 13/100: correct12.mp4_preprocessed.npy\nProcessed correct12.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 14/100: correct2.mp4_preprocessed.npy\nProcessed correct2.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 15/100: wrong48.mp4_preprocessed.npy\nProcessed wrong48.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 16/100: wrong40.mp4_preprocessed.npy\nProcessed wrong40.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 17/100: wrong34.mp4_preprocessed.npy\nProcessed wrong34.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 18/100: wrong35.mp4_preprocessed.npy\nProcessed wrong35.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 19/100: wrong9.mp4_preprocessed.npy\nProcessed wrong9.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 20/100: correct40.mp4_preprocessed.npy\nProcessed correct40.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 21/100: wrong31.mp4_preprocessed.npy\nProcessed wrong31.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 22/100: correct31.mp4_preprocessed.npy\nProcessed correct31.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 23/100: correct5.mp4_preprocessed.npy\nProcessed correct5.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 24/100: wrong17.mp4_preprocessed.npy\nProcessed wrong17.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 25/100: wrong33.mp4_preprocessed.npy\nProcessed wrong33.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 26/100: wrong16.mp4_preprocessed.npy\nProcessed wrong16.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 27/100: correct9.mp4_preprocessed.npy\nProcessed correct9.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 28/100: correct1.mp4_preprocessed.npy\nProcessed correct1.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 29/100: correct49.mp4_preprocessed.npy\nProcessed correct49.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 30/100: correct36.mp4_preprocessed.npy\nProcessed correct36.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 31/100: correct14.mp4_preprocessed.npy\nProcessed correct14.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 32/100: correct17.mp4_preprocessed.npy\nProcessed correct17.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 33/100: correct24.mp4_preprocessed.npy\nProcessed correct24.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 34/100: wrong20.mp4_preprocessed.npy\nProcessed wrong20.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 35/100: wrong6.mp4_preprocessed.npy\nProcessed wrong6.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 36/100: wrong45.mp4_preprocessed.npy\nProcessed wrong45.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 37/100: wrong27.mp4_preprocessed.npy\nProcessed wrong27.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 38/100: correct47.mp4_preprocessed.npy\nProcessed correct47.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 39/100: wrong8.mp4_preprocessed.npy\nProcessed wrong8.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 40/100: correct28.mp4_preprocessed.npy\nProcessed correct28.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 41/100: wrong29.mp4_preprocessed.npy\nProcessed wrong29.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 42/100: wrong32.mp4_preprocessed.npy\nProcessed wrong32.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 43/100: correct7.mp4_preprocessed.npy\nProcessed correct7.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 44/100: wrong28.mp4_preprocessed.npy\nProcessed wrong28.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 45/100: correct50.mp4_preprocessed.npy\nProcessed correct50.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 46/100: wrong23.mp4_preprocessed.npy\nProcessed wrong23.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 47/100: wrong38.mp4_preprocessed.npy\nProcessed wrong38.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 48/100: correct44.mp4_preprocessed.npy\nProcessed correct44.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 49/100: wrong3.mp4_preprocessed.npy\nProcessed wrong3.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 50/100: wrong15.mp4_preprocessed.npy\nProcessed wrong15.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 51/100: correct46.mp4_preprocessed.npy\nProcessed correct46.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 52/100: correct27.mp4_preprocessed.npy\nProcessed correct27.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 53/100: correct3.mp4_preprocessed.npy\nProcessed correct3.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 54/100: correct32.mp4_preprocessed.npy\nProcessed correct32.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 55/100: wrong19.mp4_preprocessed.npy\nProcessed wrong19.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 56/100: correct29.mp4_preprocessed.npy\nProcessed correct29.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 57/100: correct34.mp4_preprocessed.npy\nProcessed correct34.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 58/100: correct6.mp4_preprocessed.npy\nProcessed correct6.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 59/100: wrong26.mp4_preprocessed.npy\nProcessed wrong26.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 60/100: wrong12.mp4_preprocessed.npy\nProcessed wrong12.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 61/100: correct11.mp4_preprocessed.npy\nProcessed correct11.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 62/100: correct25.mp4_preprocessed.npy\nProcessed correct25.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 63/100: correct13.mp4_preprocessed.npy\nProcessed correct13.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 64/100: correct10.mp4_preprocessed.npy\nProcessed correct10.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 65/100: correct19.mp4_preprocessed.npy\nProcessed correct19.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 66/100: correct16.mp4_preprocessed.npy\nProcessed correct16.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 67/100: wrong13.mp4_preprocessed.npy\nProcessed wrong13.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 68/100: correct23.mp4_preprocessed.npy\nProcessed correct23.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 69/100: correct39.mp4_preprocessed.npy\nProcessed correct39.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 70/100: correct42.mp4_preprocessed.npy\nProcessed correct42.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 71/100: wrong46.mp4_preprocessed.npy\nProcessed wrong46.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 72/100: wrong25.mp4_preprocessed.npy\nProcessed wrong25.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 73/100: correct35.mp4_preprocessed.npy\nProcessed correct35.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 74/100: wrong39.mp4_preprocessed.npy\nProcessed wrong39.mp4_preprocessed.npy in 0.01 seconds\nProcessing file 75/100: correct43.mp4_preprocessed.npy\nProcessed correct43.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 76/100: correct33.mp4_preprocessed.npy\nProcessed correct33.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 77/100: wrong22.mp4_preprocessed.npy\nProcessed wrong22.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 78/100: wrong4.mp4_preprocessed.npy\nProcessed wrong4.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 79/100: wrong44.mp4_preprocessed.npy\nProcessed wrong44.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 80/100: correct38.mp4_preprocessed.npy\nProcessed correct38.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 81/100: correct4.mp4_preprocessed.npy\nProcessed correct4.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 82/100: wrong43.mp4_preprocessed.npy\nProcessed wrong43.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 83/100: wrong50.mp4_preprocessed.npy\nProcessed wrong50.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 84/100: wrong36.mp4_preprocessed.npy\nProcessed wrong36.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 85/100: correct8.mp4_preprocessed.npy\nProcessed correct8.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 86/100: wrong21.mp4_preprocessed.npy\nProcessed wrong21.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 87/100: wrong14.mp4_preprocessed.npy\nProcessed wrong14.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 88/100: wrong37.mp4_preprocessed.npy\nProcessed wrong37.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 89/100: wrong41.mp4_preprocessed.npy\nProcessed wrong41.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 90/100: correct45.mp4_preprocessed.npy\nProcessed correct45.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 91/100: wrong18.mp4_preprocessed.npy\nProcessed wrong18.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 92/100: correct26.mp4_preprocessed.npy\nProcessed correct26.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 93/100: correct37.mp4_preprocessed.npy\nProcessed correct37.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 94/100: wrong30.mp4_preprocessed.npy\nProcessed wrong30.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 95/100: wrong42.mp4_preprocessed.npy\nProcessed wrong42.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 96/100: correct21.mp4_preprocessed.npy\nProcessed correct21.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 97/100: wrong10.mp4_preprocessed.npy\nProcessed wrong10.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 98/100: correct30.mp4_preprocessed.npy\nProcessed correct30.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 99/100: wrong1.mp4_preprocessed.npy\nProcessed wrong1.mp4_preprocessed.npy in 0.00 seconds\nProcessing file 100/100: correct15.mp4_preprocessed.npy\nProcessed correct15.mp4_preprocessed.npy in 0.00 seconds\n\nNormalizing data...\nScaler saved as 'scaler.pkl'\nSplitting data into train and validation sets...\n\nData preparation completed in 0.33 seconds!\nNumber of training batches: 5\nNumber of validation batches: 2\nStarting training...\nUsing device: cuda\nEpoch [1/25]\nTrain Loss: 0.9429, Train Accuracy: 67.50%\nVal Loss: 0.6937, Val Accuracy: 55.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [2/25]\nTrain Loss: 0.5080, Train Accuracy: 85.00%\nVal Loss: 0.7722, Val Accuracy: 45.00%\n------------------------------------------------------------\nEpoch [3/25]\nTrain Loss: 0.4270, Train Accuracy: 81.25%\nVal Loss: 0.7275, Val Accuracy: 40.00%\n------------------------------------------------------------\nEpoch [4/25]\nTrain Loss: 0.2707, Train Accuracy: 93.75%\nVal Loss: 0.7660, Val Accuracy: 55.00%\n------------------------------------------------------------\nEpoch [5/25]\nTrain Loss: 0.4055, Train Accuracy: 87.50%\nVal Loss: 0.6334, Val Accuracy: 60.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [6/25]\nTrain Loss: 0.2471, Train Accuracy: 92.50%\nVal Loss: 0.5302, Val Accuracy: 75.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [7/25]\nTrain Loss: 0.3085, Train Accuracy: 87.50%\nVal Loss: 0.5387, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [8/25]\nTrain Loss: 0.1996, Train Accuracy: 92.50%\nVal Loss: 0.4908, Val Accuracy: 80.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [9/25]\nTrain Loss: 0.3135, Train Accuracy: 91.25%\nVal Loss: 0.5356, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [10/25]\nTrain Loss: 0.2633, Train Accuracy: 92.50%\nVal Loss: 0.2812, Val Accuracy: 85.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [11/25]\nTrain Loss: 0.2202, Train Accuracy: 90.00%\nVal Loss: 0.3656, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [12/25]\nTrain Loss: 0.2819, Train Accuracy: 91.25%\nVal Loss: 0.2423, Val Accuracy: 85.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [13/25]\nTrain Loss: 0.2520, Train Accuracy: 91.25%\nVal Loss: 0.2841, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [14/25]\nTrain Loss: 0.3109, Train Accuracy: 91.25%\nVal Loss: 0.2590, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [15/25]\nTrain Loss: 0.1568, Train Accuracy: 96.25%\nVal Loss: 0.1993, Val Accuracy: 85.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [16/25]\nTrain Loss: 0.2908, Train Accuracy: 91.25%\nVal Loss: 0.1771, Val Accuracy: 90.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [17/25]\nTrain Loss: 0.2276, Train Accuracy: 92.50%\nVal Loss: 0.2340, Val Accuracy: 95.00%\n------------------------------------------------------------\nEpoch [18/25]\nTrain Loss: 0.1594, Train Accuracy: 96.25%\nVal Loss: 0.1612, Val Accuracy: 95.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [19/25]\nTrain Loss: 0.0940, Train Accuracy: 96.25%\nVal Loss: 0.1285, Val Accuracy: 85.00%\nSaved new best model\n------------------------------------------------------------\nEpoch [20/25]\nTrain Loss: 0.1713, Train Accuracy: 92.50%\nVal Loss: 0.1470, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [21/25]\nTrain Loss: 0.1738, Train Accuracy: 93.75%\nVal Loss: 0.1942, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [22/25]\nTrain Loss: 0.1395, Train Accuracy: 93.75%\nVal Loss: 0.1742, Val Accuracy: 80.00%\n------------------------------------------------------------\nEpoch [23/25]\nTrain Loss: 0.1174, Train Accuracy: 95.00%\nVal Loss: 0.2621, Val Accuracy: 80.00%\n------------------------------------------------------------\nEpoch [24/25]\nTrain Loss: 0.1709, Train Accuracy: 95.00%\nVal Loss: 0.1409, Val Accuracy: 85.00%\n------------------------------------------------------------\nEpoch [25/25]\nTrain Loss: 0.1659, Train Accuracy: 95.00%\nVal Loss: 0.2407, Val Accuracy: 90.00%\n------------------------------------------------------------\nTraining completed.\nFinal trained model saved as 'final_trained_model.pth'\nTraining completed.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"!cd /\n!ls -ltra","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:43:12.519400Z","iopub.execute_input":"2024-12-03T13:43:12.520178Z","iopub.status.idle":"2024-12-03T13:43:14.601377Z","shell.execute_reply.started":"2024-12-03T13:43:12.520140Z","shell.execute_reply":"2024-12-03T13:43:14.600257Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"total 520884\ndrwxr-xr-x 5 root root      4096 Dec  3 11:30 ..\ndrwxr-xr-x 2 root root      4096 Dec  3 11:30 .virtual_documents\ndrwxr-xr-x 2 root root      4096 Dec  3 11:37 preprocessed_data\n-rw-r--r-- 1 root root       807 Dec  3 13:41 scaler.pkl\n-rw-r--r-- 1 root root 399991746 Dec  3 13:41 best_model.pth\ndrwxr-xr-x 4 root root      4096 Dec  3 13:41 .\n-rw-r--r-- 1 root root 133366830 Dec  3 13:41 final_trained_model.pth\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nfrom tqdm import tqdm\n\n# Define the model architecture (must match the training architecture)\nclass Conv3DBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=1):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = Conv3DBlock(in_channels, out_channels)\n        self.conv2 = Conv3DBlock(out_channels, out_channels)\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1),\n                nn.BatchNorm3d(out_channels)\n            )\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out = out + identity\n        return self.relu(out)\n\nclass ResNet3D(nn.Module):\n    def __init__(self, block, layers, num_classes=2, input_channels=8):\n        super().__init__()\n        self.in_channels = 64\n        self.conv1 = Conv3DBlock(input_channels, 64, kernel_size=7, stride=2, padding=3)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, out_channels, blocks, stride=1):\n        layers = []\n        layers.append(block(self.in_channels, out_channels))\n        self.in_channels = out_channels\n        for _ in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = x.unsqueeze(2).unsqueeze(-1)  # Shape: [batch_size, channels, 1, sequence_length, 1]\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\ndef resnet3d18(num_classes=2, input_channels=8):\n    return ResNet3D(ResidualBlock, [2, 2, 2, 2], num_classes, input_channels)\n\n# Load the trained model\nmodel = resnet3d18(num_classes=2, input_channels=8)\nmodel.load_state_dict(torch.load('final_trained_model.pth', map_location=torch.device('cpu')))\nmodel.eval()\n\n# Load the scaler\nscaler = joblib.load('scaler.pkl')\n\n# Initialize MediaPipe Pose for pose estimation\nmp_pose = mp.solutions.pose\npose = mp_pose.Pose(static_image_mode=False,\n                    min_detection_confidence=0.5,\n                    min_tracking_confidence=0.5)\n\ndef calculate_angle(a, b, c):\n    \"\"\"Calculate the angle between three points.\"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    c = np.array(c)\n    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - \\\n              np.arctan2(a[1] - b[1], a[0] - b[0])\n    angle = np.abs(radians * 180.0 / np.pi)\n    if angle > 180.0:\n        angle = 360 - angle\n    return float(angle)\n\ndef extract_features(frame):\n    \"\"\"Extract features from a single frame.\"\"\"\n    return [\n        frame['left_arm_angle'],\n        frame['right_arm_angle'],\n        frame['landmarks'][11][1],  # LEFT_SHOULDER y-coordinate\n        frame['landmarks'][12][1],  # RIGHT_SHOULDER y-coordinate\n        frame['landmarks'][13][1],  # LEFT_ELBOW y-coordinate\n        frame['landmarks'][14][1],  # RIGHT_ELBOW y-coordinate\n        frame['landmarks'][15][1],  # LEFT_WRIST y-coordinate\n        frame['landmarks'][16][1]   # RIGHT_WRIST y-coordinate\n    ]\n\ndef preprocess_video(video_path, sequence_length=100):\n    \"\"\"Preprocess the video and extract features.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    preprocessed_data = []\n    frames = []  # Store frames for visualization\n\n    for frame_idx in tqdm(range(frame_count), desc=\"Processing Video\"):\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frames.append(frame.copy())  # Save the frame for later visualization\n\n        # Convert the image to RGB and process it with MediaPipe\n        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        results = pose.process(image_rgb)\n\n        if results.pose_landmarks:\n            landmarks = results.pose_landmarks.landmark\n\n            # Extract relevant landmarks (e.g., shoulders, elbows, wrists)\n            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n                             landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x,\n                              landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]\n            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n                          landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n            right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].x,\n                           landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value].y]\n            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].x,\n                          landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y]\n            right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].x,\n                           landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y]\n\n            # Calculate angles\n            left_arm_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n            right_arm_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n\n            preprocessed_data.append({\n                'frame': frame_idx,\n                'left_arm_angle': left_arm_angle,\n                'right_arm_angle': right_arm_angle,\n                'landmarks': [(landmark.x, landmark.y, landmark.z) for landmark in landmarks]\n            })\n        else:\n            # If no landmarks detected, append zeros (or handle accordingly)\n            preprocessed_data.append({\n                'frame': frame_idx,\n                'left_arm_angle': 0.0,\n                'right_arm_angle': 0.0,\n                'landmarks': [(0.0, 0.0, 0.0)] * 33  # Assuming 33 landmarks\n            })\n\n    cap.release()\n    return preprocessed_data, frames\n\ndef visualize_results(frames, predicted_class, confidence):\n    \"\"\"Overlay the prediction on video frames and save the result.\"\"\"\n    # Define the label and color based on the prediction\n    if predicted_class == 1:\n        label = f'Correct Form ({confidence * 100:.2f}%)'\n        color = (0, 255, 0)  # Green for correct\n    else:\n        label = f'Incorrect Form ({confidence * 100:.2f}%)'\n        color = (0, 0, 255)  # Red for incorrect\n\n    # Initialize video writer\n    height, width, _ = frames[0].shape\n    output_path = '/kaggle/working/output_video.mp4'\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'),\n                          20, (width, height))\n\n    for frame in frames:\n        # Overlay text on the frame\n        cv2.putText(frame, label, (50, 50), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.5, color, 3)\n        out.write(frame)\n\n    out.release()\n    print(f\"Visualization saved as {output_path}\")\n\ndef main():\n    # Path to your new video file\n    video_path = '/kaggle/input/test-pushup/pushup_002.mp4'  # Update this path\n\n    # Preprocess the video and extract features\n    preprocessed_data, frames = preprocess_video(video_path)\n\n    # Extract features from preprocessed data\n    features = np.zeros((len(preprocessed_data), 8))\n    for i, frame in enumerate(preprocessed_data):\n        features[i] = extract_features(frame)\n\n    # Normalize features using the saved scaler\n    features_normalized = scaler.transform(features)\n\n    # Standardize sequence length\n    sequence_length = 100  # Same as during training\n    if len(features_normalized) < sequence_length:\n        pad_length = sequence_length - len(features_normalized)\n        features_normalized = np.pad(features_normalized, ((0, pad_length), (0, 0)), mode='constant')\n    elif len(features_normalized) > sequence_length:\n        features_normalized = features_normalized[:sequence_length]\n\n    # Prepare input tensor\n    input_tensor = torch.FloatTensor(features_normalized).permute(1, 0).unsqueeze(0)\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(input_tensor)\n        probabilities = torch.softmax(output, dim=1)\n        predicted_class = torch.argmax(probabilities, dim=1).item()\n        confidence = probabilities[0, predicted_class].item()\n\n    # Output result\n    if predicted_class == 1:\n        print(f\"The push-up form is **correct** with confidence {confidence * 100:.2f}%\")\n    else:\n        print(f\"The push-up form is **incorrect** with confidence {confidence * 100:.2f}%\")\n\n    # Visualize the results\n    visualize_results(frames, predicted_class, confidence)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:52:47.730966Z","iopub.execute_input":"2024-12-03T13:52:47.731456Z","iopub.status.idle":"2024-12-03T13:52:50.023342Z","shell.execute_reply.started":"2024-12-03T13:52:47.731424Z","shell.execute_reply":"2024-12-03T13:52:50.022332Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/415117396.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('final_trained_model.pth', map_location=torch.device('cpu')))\nProcessing Video:   0%|          | 0/42 [00:00<?, ?it/s]W0000 00:00:1733233968.295797    1434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1733233968.350436    1434 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nProcessing Video: 100%|██████████| 42/42 [00:01<00:00, 31.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"The push-up form is **correct** with confidence 75.50%\nVisualization saved as /kaggle/working/output_video.mp4\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"!cp final_trained_model.pth scaler.pkl /kaggle/working/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T13:55:34.999492Z","iopub.execute_input":"2024-12-03T13:55:35.000439Z","iopub.status.idle":"2024-12-03T13:55:36.053276Z","shell.execute_reply.started":"2024-12-03T13:55:35.000403Z","shell.execute_reply":"2024-12-03T13:55:36.052316Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"cp: 'final_trained_model.pth' and '/kaggle/working/final_trained_model.pth' are the same file\ncp: 'scaler.pkl' and '/kaggle/working/scaler.pkl' are the same file\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"input_folder = '/kaggle/working/preprocessed_data'\nsequence_length = 100\nbatch_size = 16\nnum_epochs = 50\nlearning_rate = 0.001\nweight_decay = 1e-5\nprint(\"Done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}